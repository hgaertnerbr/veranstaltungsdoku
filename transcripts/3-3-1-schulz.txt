Ich würde euch gerne auch noch sehen im Raum, aber irgendwie bekomme ich das gar nicht auf die Reihe. Also im Moment, außer mir ist noch die Alinde da. Ich verlasse jetzt hier auf den zweiten Minuten das Gespräch. Ja, ja. Ich funktioniere sehr gut mit dem konkreten Arbeitsmarkt. Ich habe jetzt noch nicht gehört. An der Stelle können wir beide drauf sein. Eine Frage an euch. Was braucht ihr für eine Entscheidung? Ich höre den Satellit Hamburg sehr laut und deutlich. Aus unserer Sicht. Hamburg, achte jetzt hier bitte darauf, dass an einem Gerät nur der Lautsprecher und Mikrofon verwendet wird. Ich habe die Stimme geschalten. Entschuldigung, ich habe die Stimme. Die Schellen uns nicht zu hören. Ja, hören tun sie dann noch. Es war nur, weil da ist das Pfeifenkommun, da war die Rückköpplung. Okay, auch hier füllt sich dann der Saal langsam die einen oder anderen Treffen noch ein. Das ist ja schön, dass es den einen oder anderen interessiert. dann genau ja, hoffentlich ich guck mal, okay, ist alles richtig schön Technik funktioniert ich kann bloß euch und meine Präsentationsansicht meine Moderatingsansicht nennt sich das im PowerPoint Egal. Die kann ich nicht gleichzeitig sehen. Das ist ein bisschen irritierend. Mach es doch über PowerPoint live. Also wenn du PowerPoint auf hast, die Präsentation, dann müsste oben rechts ein Button sein, in Teams teilen. Und dann kriegst du als Presenter quasi diese ganze Vorsicht, Voransicht. Entschuldigung, ich habe mich doppelt. Liegt aber an mir. Und dann kriegst du das Presenter an sich. Das kann sein, dass es nur in diesem Business Microsoft geht. Ich bin ein Privat-Office-User. Wo steht denn diese Funktionsmöglichkeit? Schiff mal bitte dein PowerPoint auf den geteilten Bildschirm. Moment, was teile ich denn jetzt gerade? Moment. Ich habe jetzt bei mir nur den Hinweis ausschalten müssen. Oh Gott, ja da oben irgendwo, ist da eins runter. Da wo dieser orange Button oder so ist, da müsste ich das von da aus nicht lesen, aber da war bei mir. Ja, aber da steht es so nicht drin. Okay. Ja, ich vermute, es gibt es einfach wahrscheinlich nur in der Besinnungsvariante. Okay, ich gehe mal wieder auf Präsentationsmodus und teile euch das so. Achso, ihr seht das jetzt wieder, oder? Ja. Perfekt, dann lasse ich das so eingestellt, wie es jetzt ist. Dann, ich möchte das Fenster ein bisschen kleiner, dann kann ich euch auch irgendwie sehen. Das ist halt nicht so groß. Okay, dann schön, dass ihr alle da seid. Das freut mich natürlich sehr. Und ich fange einfach mal direkt an, weil wir haben ja jetzt schon zwei Minuten Zeit verloren. Das muss man wieder gut machen. Genau, vielleicht, ich wollte, also mein Plan ist, euch einen kurzen Impuls zu geben und das Thema einzuführen. Und danach werden wir alle tätig. Und ich teile euch dann auch noch einen Link, damit wir tätig werden können. Zum einen ist das Thema, der Titel meiner Präsentation ja meint, Diversity Gap. Und dazu wollte ich euch erstmal herleiten, das Gender Data Gap ist ja viel von der Begrifflichkeit vermutlich präsenter als das Diversity Gap. Deswegen würde ich erstmal das erklären, auch für diejenigen, die vielleicht noch nicht davon gehört haben. Es gibt ja ein sogenanntes Gender Data Gap, also eine systematische Datenlücke, geschlechterbezogen in Datenerhebungen, Studien, Normen, technischen Entwicklungen. Ganz besonders präsent oder immer mal wieder bekommt man davon mit, auch in der Medizin oder bei der Normung von Möbeln und so, dass so quasi die Standardstatut von einem Mann genutzt wird und solchen Sachen. Und das nennt man dann eben dieses Gender Data Gap und analog dazu bezeichnet das Diversity Gap die Lücke zwischen dem Anspruch auf Vielfalt. Viele Organisationen sagen ja schon, ja, wir bekennen uns zu Vielfalt, wir wollen Vielfalt. und die Lücke zwischen diesem Anspruch eben und der tatsächlichen Repräsentation von diesen Gruppen in Organisationen und Entscheidungsprozessen und so weiter. Das heißt also zum Beispiel, dass wir wenig Frauen oder Menschen mit Migrationsgeschichte oder Ähnliches in Führungspositionen haben Oder dass People of Color, queere Menschen in Studien, Filmen, Lehrplänen etc. pp. unterpräsentiert sein können. Oder dass eben auch diese marginalisierten Gruppen an Entscheidungsprozessen weniger beteiligt sind oder werden. Ist das soweit erstmal von der Begrifflichkeit klar oder hat da jemand noch? Ja, ich sehe Kopfnäten, das ist sehr schön. Genau und woran liegt das jetzt überhaupt erstmal, diese strukturelle Verankerung? Wie kommt es zu diesem Gap? Das liegt eigentlich mehr oder weniger mit in der Natur des Menschen, weil unser Gehirn einfach effizienter funktioniert, wenn es Dinge in Schubladen packen kann. Deswegen haben wir Rollenbilder, deswegen haben wir Stereotype, also verallgemeinernde Zuschreibungen, die in der Regel unbewusst entstehen, unbewusste Vorteile oder eben dann daraus resultierend auch aktive oder strukturell verankerte Benachteiligungen. Genau, was hat das jetzt alles erstmal mit KI zu tun? Moment, ich muss noch mal kurz einen Satz zurück. Also diese Gruppen, die nämlich systematisch unterpräsentiert sind oder verzerrt dargestellt werden, ist mittlerweile nachgewiesen tatsächlich, ich habe hier ein Beispiel mitgebracht, dass KI das eben nicht nur spiegelt, sondern sogar verstärkt. Ich habe ein Beispiel dabei, wen es interessiert, dem kann ich später, also den Link zu dieser Studie, aber auch zu noch einer weiteren Studie zukommen lassen, also gerne mich ansprechen. Es ist mehr oder weniger absichtlich auch schlecht lesbar, weil es lässt sich einfach in dieser Fülle nicht groß darstellen, aber ich kann euch die Grafik mal kurz erklären. In dieser Studie, die hier gemacht wurde, wurden Bilder von DALI 2 damals noch erzeugt zu Berufsbildern und dann eben untersucht, wie viel Prozent davon Männer dargestellt wurden und wie viel als Frauen. Und hier hat man die Farbgebung auch wieder sehr stereotyp gewählt. Also orange steht an der Stelle hier für die Darstellung von Frauen, zum Beispiel Sängerin, Lehrerin, Krankenschwester und so weiter. Alles, was ihr hier komplett orange seht, sind Berufe, wo Dali zwei nur Frauen generiert hat. Und dann gibt es auch ein paar Berufsgruppen, wie zum Beispiel Ärzte oder Köche oder so, wo dann auch so eine Mischung stattfindet. Aber noch relativ unausgewogen sind ganz viele Berufe, die in diesen Bildern tatsächlich rein männlich dargestellt wurden. Und da gibt es auch noch verschiedene Studien, nicht nur zur Geschlechterverteilung, sondern zum Beispiel auch zur Verteilung von Stereotypen bei kulturellen Aspekten, Nationalität und so weiter. Also da gibt es auch Studien dazu. Woran liegt es jetzt, dass KI diese Aspekte sogar verstärkt? Das liegt, wie eingangs gesagt, natürlich auch an unserem, wie unser Gehirn funktioniert, an unserem Schubladendenken, aber eben auch an mehreren anderen Sachen. Also zum einen haben wir bedingt durch strukturelle, man kann sagen auch Diskriminierung, bestimmte Berufe durften nur Männer ausüben. Männer war damals der Zugang zur Bildung leichter, Männer durften eher studieren, Männer durften eher politisch aktiv werden und sowas alles. Also wir haben halt historische Daten und Vorurteile und jetzt auch in den letzten Jahren und Jahrzehnten wird halt auch immer bekannter, dass es auch viele Frauen gab, die aber eben aufgrund dieser Forschung, also in der Wissenschaft, aber auch in der Kunst, die dort tätig wurden, aber eben nie erwähnt wurden aus diesen Gründen. Das heißt, wir haben viele historische Daten und Vorurteile, die dem Ganzen zugrunde liegen, mit der unsere KI trainiert wird. Dann gibt es eine Subjektivität in der Datenkuration. Wenn ich jetzt hier ein Team bin aus Europa mit bestimmten Einflüssen und wir bestimmen jetzt, wie die KI trainiert wird, dann legen wir den Regeln zugrunde, mit denen wir groß geworden sind. Entschuldigung, die Hitze, die geht mir ein bisschen auf die Stimme. Keiner sagt mir, dass meine Kamera aus ist. Entschuldigung, ich sehe es gerade erst. Habt man das wie Absicht gewesen? Nein, das war keine Absicht. Keine Ahnung, vielleicht liegt es auch in der Hitze, ich weiß es nicht. Wir haben eine Subjektivität in der Datenkuration, aber eben auch eine kulturelle Kontext. Interpretation. Dann gibt es auch immer noch algorithmische Verzerrungen, die dann tatsächlich am Algorithmus liegen. Oder auch Feedback-Schleifen. Wenn eine KI trainiert wird, dann gibt man, analysiert die Daten, dann gibt es irgendwann nochmal eine Feedback-Schleife, wo man guckt, was kommt da für ein Output. Das wird dann natürlich das Feedback wieder vom Menschen gegeben. Der Mensch ist halt nur mal voreingenommen, ob er will oder nicht. Und so führt das halt dazu, dass es eben nicht nur wiedergespiegelt wird, sondern verstärkt wird. Ich hoffe, hier ist bis dahin auch erstmal soweit alles klar oder gibt es dazu Fragen, Diskussionsbeiträge? Nein. Okay. Danke. Genau. Diese ganzen verzerrten Daten sind am Ende dann quasi auch verzerrtes Wissen und für die KI existiert eben das, was wir messen oder was eben nicht in diesen Daten steht, existiert für die KI eben nicht. Das hat man gestern in dem Beispiel mit den Eiern gesehen, für die KI sind die Eier halt Eier und irgendwo gibt es zwar Wissen darüber, dass Eier kaputt gehen, wenn sie runterfallen, aber das war für die KI an der Stelle halt eben nicht so eindeutig, weil die eben nicht nach diesem ganzen Wissen gräbt und das nicht alles so verknüpft wie wir. Genau. Und wenn es halt keine Bilder gibt von kaputten Eiern, dann wer postet denn schon solche Missgeschicke, wo ein Haufen kaputte Eier am Boden liegen, gibt es vielleicht auch fehlendes Datenmaterial, was die KI verwenden könnte. Und ich habe jetzt mal Beispiele mitgebracht. Das erste Beispiel ist noch ein bisschen ausführlicher, danach nur noch mal kurz. Und dann gehen wir noch in eine kurze Übung rein, weil ich sehe schon, die Zeit, die rennt ganz schön. Die Hitze macht mich vielleicht ein bisschen langsamer als sonst. Also wir haben typisch Rollenbilder, Frauen als Fürsorgerinnen, Männer als Hauptverdiener, Jugendliche sind technikaffin, wahlweise oder auch mal unmotiviert. Und solche Klischees führen dann eben auch dazu, dass Menschen sich selber auch auf diese Rollen beschränken, die einem vorgegeben werden. Und deswegen ist es ein Problem und deswegen ist eins meiner Ziele zum Beispiel eben dagegen vorzugehen, indem man sich das einfach bewusst macht. Und ich habe eine Bildbeschreibung genommen, die habe ich dann noch ins Englische übersetzt, bevor ich sie in die KI gegeben habe. Eine Person, die ein Kleinkind auf dem Arm hält und sich gleichzeitig in einer Küche um den Haushalt kümmert. Was meint ihr? Ist diese Formulierung neutral? Könnt ihr mal Daumen hoch machen, wenn ihr sagt, ja, ist aus meiner Sicht neutral die Formulierung. Ja, hier kommen Daumen hoch, genau. Klang für mich auch so weit neutral, genau. Und was denkt ihr denn, wie gängige KI-Tools das darstellen? Vermutungen, wenn jemand das sagt? Ich im Raum höre ich ein bisschen. Ja, eine Frau mit vermeintlich langen oder kurzen Haaren im Rock vermutlich, mit dem Baby auf dem Arm. Da sagen wir Gänzen so knapp 30. also definitiv nicht über 40. Ja, okay, gut. Ich zeige euch jetzt Bilder. Ich habe verschiedene KIs genommen. Wir fangen mal an mit einem relativ aktuellen MidJourney Model Kind. Das ist ein Modell von MidJourney 6.1. Es gibt mittlerweile ein neueres Modell. Und da war ich, muss ich sagen, ehrlich gesagt schwer enttäuscht, weil das Modell davor war schon mal diverser. Vielleicht, wenn ihr das Bild mal genauer betrachtet, Die haben alle Röcke oder Schürzen. Was mir besonders noch an diesen Bildern aufgefallen ist, die sind alle dem Kind zugewandt, die machen gar nichts nebenbei im Haushalt. Also ich habe wirklich, also das ist nur eine Auswahl aus Bildern. Ich habe halt mit diesem Prompt 40 Bilder erzeugt und dann mal eine kleine Auswahl daraus genommen. Dann habe ich ein älteres Modell gehabt, das war der Grund meiner Enttäuschung, da kamen ab und zu auch mal Bilder von Männern, die allerdings dann auch immer lange Haare und Zöpfer hatten, aber doch dann schon männlich aussahen, sieht man im ersten Bild, kam vereinzelt vor und auch hier ist eigentlich die Person immer mit dem Kind beschäftigt, nicht mit dem Haushalt. Mit Journey 7, jetzt nur der Vollständigkeit halber, hat es verbessert. Da habe ich wirklich Unterschiede, nicht im Alter und auch nicht in der Situation an sich gesehen, aber durchaus in der Repräsentanz von Vätern. Bei 40 Bildern waren tatsächlich 10 Väter dabei und sie hatten auch alle so unterschiedliche Hautfarben, also stammten auch aus unterschiedlichen Nationalitäten in den Bildern, was vorher nicht der Fall war. Und jetzt, was meint ihr, wie wird, wenn ich in ChatGPT diesen Prompt eingebe, von DALI 3 das Bild ausgegeben? Ist das Bild genauso oder bekommen wir ein anderes Bild? Also ich denke, es ist genau noch gleich, weil DALI immer in die gleiche Richtung fährt. Für mich, Johnny, könnte ich mir das vorstellen. Also ich bin mir nicht ganz sicher. Die sind ja ins Web abgewandert von Discord-Servern. Dass vielleicht dadurch die Datengrundlage dann ein bisschen anders bauer ist. Aber bei DALL-E glaube ich immer noch, dass es bauer sind. Okay. Ich glaube, danke für das Feedback. Ich denke, wenn sie keinen Filter davor geschalten hätten, wäre es tatsächlich so. Aber in ChatGPT ist tatsächlich ein Filter eingebaut, der dafür sorgt, wenn ich Prompt bewusst neutral formuliere, dass er dann absichtlich extrem viel Diversität oder gegensätzliche Bilder reinbringt. Also es ist mir wirklich sehr, sehr schwer gelungen, auch mal eine Frau zu produzieren mit diesem Prompt. Das war quasi nicht möglich. Also das sieht man hier und ich weiß nicht, ich habe es vorhin so betont, die Situation, wie die dargestellt wurden, was bei DALI 3 jetzt auch nochmal besonders auffällt. Die Männer mit dem Kind auf den Armen, die tun tatsächlich was im Haushalt. Und das fand ich sehr interessant und frage mich, woher das kommt. Genau, aber wir wollen noch kurz in die Übungen. Ich habe der Vollständigkeit halber noch mal kurz andere Modelle, aber die überspringe ich jetzt mal kurz. Das dauert sonst zu lange. Ich habe mal noch andere interessante Bilder mitgebracht, die wieder mit Journey Modell 6.1 gemacht sind. Eine Person in einem formellen Bürooutfit an einem Konferenztisch. Und ganz ehrlich, wenn ich von einem Konferenztisch rede, gehe ich eigentlich davon aus, dass an diesem Konferenztisch mehrere Personen sitzen. Aber vielleicht liegt es auch tatsächlich an der Formulierung, dass ich von einer Person an einem Konferenztisch gesprochen habe, das wirklich sehr exakt dargestellt wurde. Aber genau, ich habe keinen Konferenztisch bekommen, an dem wirklich mehrere Personen saßen. Aber etwas diverseres Bild muss man wirklich zugeben als in der Haushaltssituation mit dem Kind auf dem Arm. Und nochmal ein ganz gegensätzliches Bild, eine Person an Arbeitskleidung auf einer Baustelle mit handwerklichen Tätigkeiten. Zugegeben beim letzten Bild sieht man jetzt nicht, welches Geschlecht diese Person hat. Solche Bilder gab es vereinzelt auch, aber die meisten Bilder waren dann tatsächlich so, dass es doch schon sehr wenig gelesen aussieht. Jetzt haben wir noch zehn Minuten für eine Übung. Jetzt überlege ich gerade, wie ich es mache. Oder ich erkläre euch einfach, was ich normalerweise in diesem Workshop mache, wenn ich nicht so lange und ausführlich rede. Ich habe eine Pinnwand für euch vorbereitet und ich poste euch mal jetzt den Link in den Chat. Ah toll, dass er das jetzt macht. Er macht mir den Link jetzt als Bild. Woran kann das liegen? Das haben wir den nicht als Text kopiert. Hat jemand eine Idee? Probier mal Steuerung Shift-V. Du hast den Bildschirm geteilt, die Präsentation, oder? Der hat mir das gleichzeitig irgendwie als Bild eingefügt, den geteilten Link. Ich habe eine Pinnwand und normalerweise, wenn ich diesen Workshop mit Schülern habe, sammle ich dort Rollenbilder. Dann schreiben die da Rollenbilder rein und die überarbeiten wir dann. Wir können uns ja mal fünf Minuten geben oder wir geben uns mal drei Minuten. Es kann ja mal jeder ein einzelnes Rollenbild in einer dieser Spalten eintragen, in dieser Pinnwand, damit wir eine Arbeitsgrundlage haben. Normalerweise gebe ich dafür zehn Minuten, wir nehmen es drei Minuten. Tragt mal jeder ein Rollenbild ein, was euch einfällt. Also ein Klischee, es darf wirklich sehr klischeebehaftet sein. Dann sehe ich das hier auch in der Pinnwand. Konntet ihr den Link alle eröffnen? Oder hat da noch jemand Probleme mit? Vielen Dank. So, jetzt schiebe ich mal die Kühne darüber, damit ihr das auch seht, was da so eintrudelt. 20 sekunden Zeit, zum Ende zu tippen. Jetzt kam ein ganzer Schwung rein. Genau, jetzt ist die Zeit rum. Und im nächsten Schritt, für den ich normalerweise auch etwas mehr Zeit gebe, wäre die Frage, beziehungsweise eigentlich sind es drei Fragen. Wie kann man dieses Klischee neutral formulieren? Wie könnte man es aufbrechen? Und welche alternativen Darstellungen oder Bilder würden zeigen, dass dieses Rollenbild nicht immer zutrifft? Und jetzt habt ihr zwei Möglichkeiten. Ihr könnt im nächsten Schritt entweder selber euren Kopf anstrengen. Moment, ich muss mal meine Maus hier rüber auf dem anderen Bildschirm. Entweder selber euren Kopf anstrengen und euch das überlegen oder ihr könnt den oberen oder den unteren Link klicken. Ich habe auf den oberen Link, den ich da längst eingefügt habe, in der Pinnwand, ein Tool für die, die keine KI zur Hand haben oder installieren dürfen. Das ist ein kostenloses datenschutzkonformes Tool. Also kostenlos für die, die es benutzen, nicht für mich. Und darunter habe ich einen Diverse-Promthelfer und hier geht es zurück zu der PIN-Band. Den könnt ihr benutzen oder aber den zweiten Link, den ich eingeführt habe, das ist von ChatGPT, einen Assistenten, den ich mal gefüttert habe. den habe ich Diversity Gap Feiner genannt und den fragen, wie könnte man das Klischee dann aufbrechen und nach einem besseren Prompt fragen, aus dem man dann ein Bild macht, bestenfalls. Aber ich glaube, zum Bild generieren haben wir dann am Ende keine Zeit mehr. Wir machen jetzt einfach mal nur noch Textungen. Also ihr könnt eine dieser Fragen benutzen, um den Prompt selber umzuformulieren, diese Eingabe, dieses Klischee, dieses Rollenbild. Oder ihr nutzt einen dieser beiden KI-Assistenten, was euch lieber ist. Wenn ihr sagt, Chat-Japti-Assistent will ich machen oder ich nutze lieber das Infobis, ist euch das freigestellt. Und dafür gebe ich euch jetzt nochmal, ja, haben wir auch bloß noch einmal drei Minuten, das tut mir leid. Ich habe zu lange redet zu Beginn. Das hat sich jetzt nicht geklappt. ... Ich kann euch ja nochmal die Fragen einblenden, damit ihr die auch seht. Und vielleicht bekomme ich es gleich hin, euch das auch nebeneinander zu packen. Die Pinnwand und das tun. Ach, danke Oliver. Ich sehe gerade, du hast auch einen Link, einen Custom GPT gebaut und in dem Chat geteilt. Genau und diese Ergebnisse, die ihr dann habt mit der Überarbeitung, die dann gerne direkt als Kommentar oder Ergänzung in der Pinnwand teilen, wo ihr vorher schon geschrieben habt, damit man das noch zuordnen kann. Die Pinnwand selber ist auch noch länger zugänglich, nur der Klassenraum sozusagen mit den Trolls dann leider nicht mehr. So, und jetzt haben wir noch drei Minuten, um das hier zu beenden. Ihr könnt ja mal noch fix zu Ende schreiben. Ich teile das mal wieder. Genau, ich sehe schon, da sind schon Ergebnisse angekommen. Ich hole mir die einmal mal hier rüber, damit ich die sehen kann. Ihr seht die ja bei euch selbst auch. Da kommen dann gute Ergebnisse, sehr schön. Genau. Wie gesagt, es tut mir leid, dass ich am Anfang zu lange geredet habe. Ich würde mich eigentlich kürzer fassen. Was jetzt noch meine letzte Frage an euch ist für die letzten zwei Minuten. Was hat euch überrascht und wie könnt ihr das Gelernte in Zukunft anwenden? Vielleicht mögt ihr da noch ein Stimmungsbild zugeben. Das hat alle noch so vertieft, ne, ins Prompten. Aber die Session ist leider gleich vorbei. Achso, wir haben noch einen Moment, ne, wir haben 15 angefangen. Ja, die Session geht offiziell bis 45. Ach Gott, ja, dann gebe ich euch nochmal ein paar Minuten, dann promptet mal noch eine Runde weiter. Ich war gerade irgendwie gedanklich da, oh Mist, wo ist die Zeit hin? Wir müssen um 45 fertig sein, es tut mir leid. Die Hitze, die stecken. Wie viel Grad habt ihr es? Ich habe hier 29 Grad in meinem Wohnzimmer, deswegen... Moment, ich schaue mal noch mal kurz ins Programm. Bis 14 Uhr? Oh doch, ich sage gerade, es geht bis 14 Uhr. Stimmt, wir haben um 13.15 Uhr gefangen. Genau, dann gebe ich euch mal noch 5 Minuten. Ihr könnt eure Gedanken noch fertig ausführen, wenn ihr wollt. Über ChatGPT selber oder über den Link, den ich euch geteilt hatte, könnt ihr auch ein KI-Tools für Bild-KI benutzen. Könnt ihr auch noch Bilder prompten und dazu packen, wenn ihr mögt. Ich gebe euch noch mal fünf Minuten. Ich war gerade durch die Hitze echt, die haut mich hier heute vom Hocker. Ich werde hier 38 Grad erwartet. Wir haben es schon 35 draußen. Also im Banker-Street-Raum sind es 30. Ja. Dann geht es euch nicht besser wie mir. Wir haben aber einen riesigen Ventilator. Ja, meiner ist nicht ganz groß. Ich drehe mir die Kamera. Ja, schön. Der ist ein bisschen größer als meiner. Noch ein bisschen stiller Arbeit. Und wer fragen hat mit der Pinnwand, wie das da funktioniert, mit einem Link oder einem Bild oder sowas hinzufügen, einfach reinfragen. Oder ich habe es versucht, auch in der linken Spalte zu beschreiben, in der allerersten Spalte in der Pinnwand. Vielen Dank. Wir haben eigentlich bewusst auf Bilder gesetzt, also dass wir Bilder prompten und nicht Texte bzw. daran denken, wir wollen daraus ein Bild erzeugen. Bei Bilder das natürlich bildhafter darstellen als Texte. Bei Texten muss man manchmal nach diesen Aspekten suchen und bei Bildern ist es dann schon schneller offensichtlich und mit einem Blick zu erfassen. 't't Noch eine Minute. Die Zeit ist abgelaufen. Dann jetzt gehen wir wirklich einen Schritt weiter. Was hat euch überrascht? An den Ausgaben von der KI. Und wie könnt ihr das Gelernte von heute, also von dieser Session, in Zukunft anwenden? Das interessiert mich jetzt und gerne könnt ihr auch noch andere Impulse geben, Fragen stellen. Genau, wir haben jetzt noch zehn Minuten Zeit zu diskutieren. Möchte jemand was sagen? Wer hat denn schon ein Bild generiert und wer hat das Text? Da hinten wird der Würfel gereicht, okay. Genau, hi, ich bin Sandra. Ich habe jetzt mal ein paar Bomben zerstellt, was relativ schnell und gut ging. Die haben sich auch gut gelesen. Dann habe ich es quasi in die Bildgenerierung rein. Also Care-Arbeit und genderneutral. Es sollten Leute mit Laptop arbeiten, aber jetzt schauen leider alle in den Laptop mit Gender und keiner macht Care-Arbeit. Genau, das war meine Erkenntnis. Okay, was ist das noch? Es ist auch relativ einfach, die KI dafür zu nutzen, um das zu versuchen. um etwas neutraler zu gestalten. Genau, und um das dann wirklich so fein zu feilen, dass es so angezeigt wird, bräuchte man wahrscheinlich noch ein bisschen Zeit, ne? Ja, da muss man noch ein bisschen rumfeilen. Ja, okay. Wer hat noch eine überraschende Erkenntnis gewonnen? Achso, sorry. Ich kann doch gerne was anderes. Ja, hi. Genau, und ich habe Azure genutzt. Das haben wir für das Intern und habe Hebamme eingegeben, weil das kann ja auch ein Mann sein. Ich habe es gerade noch mal gegoogelt. Das ist auch irgendwie die Bezeichnung für eine männliche Hebamme. Und habe das nur eingegeben. Und es kam tatsächlich ein relativ, also da wurde auf jeden Fall schon der Diversity-Filter draufgelegt, weil es kam tatsächlich ein schwarzer Mann raus, der das ist. Genau. Und das ist ja jetzt quasi eigentlich nicht das, was man sagt, das ist typischerweise das. Genau. Dann habe ich nochmal nach der typischen Hebamme gefragt und dann waren es auch verschiedene Personen tatsächlich. Aber jetzt allein mit dem prompt, zeigt mir eine Hebamme, ist das schon ziemlich gut gewesen, was jetzt das Thema Diversity angeht. Es ist halt die Frage, ob das dann das realistische Abbild quasi ist. Aber auf jeden Fall im Sinne der Diversity ganz gut. Das fand ich entspannt, dass da auf jeden Fall ein Filter drauf ist. Der Oliver hat gerade die Hand gehoben? Oder habe ich das falsch interpretiert? Nein, Sven daneben. Ah, okay. Ich dachte, ich hätte es im Bild gesehen, aber gut, dann hat er vielleicht wegen dem Würfel die Hand gehoben. Oli kommt auch gleich dran. Ich habe einfach nur ganz kurz zwölf Bilder von also ich habe es in Englisch formuliert und dann denke ich immer, da ist der Diversity-Faktor automatisch mit den Truck-Driver, LKW-Fahrer. Truck Driver in front of der Truck. Und da hat es tatsächlich von den zwölf Bildern immerhin drei Frauen. Also von dem her, also ich versuche das da immer, wenn ich so einen Diversity-Vergleich mache, gleich eine Vorgabe zu machen, gibt mir eben zehn Bilder, zwölf Bilder, 16 Bilder. Da kann man ja das ein bisschen vergleichen. An der Stelle gesagt, bei Truck Driver, ich habe Perplexity genutzt. Was nicht populär ist, jetzt sage ich mal direkt für Bildgenerierung, dauert immer ein bisschen länger. auch, wie gesagt, hat von den 12 an drei Frauen. Danke, spannend. Soll ich da was sagen? Ja, gerne. Ich war überrascht, dass da jeder Eingriffe hat, wo du auch vorher gezeigt hast, dass ja wirklich dann Eingriffe war, ist eine Programmierung. Wobei ich mir immer frage, alles, was zusätzlich Eingriffe werden muss und so gerichtet werden muss, weil es nicht dem normalen Weltbild entspricht, kann irgendwann mal Probleme erzeugen, denke ich. Weil im Prinzip wäre es ja wünschenswert, dass unsere Datengrundlage im Internet so divers wäre, dass das, was rauskommt, dem entsprechen wird. Und das ist ja nicht. Man muss ja immer wieder eingreifen. Da bin ich mal gespannt, was das für Spätfolge in deiner Large Language Modelle oder Fusion Modelle erzeugen. Ja, richtig. Aber wie gesagt, es hat ja halt auch die Ursache in den historischen Daten, in der historischen Datengrundlage, die dem zugrunde liegen oder auch, was postet man in sozialen Medien? Postet man da die schönen Dinge oder stellt man sich selber und alles so da, wie es tatsächlich ist? Man versucht ja eigentlich schon von sich aus Fotos immer bestmöglich aussehen zu lassen. auch wenn es am Ende darum geht, irgendwas Negatives darzustellen. Man gibt sich Mühe, es ordentlich darzustellen und macht halt nicht einfach nur einen Schnappschuss. Deswegen ist es halt sehr, sehr schwierig, eine wirklich ausgewogene Datengrundlage zu schaffen. Mir ist gerade noch was eingefallen. Ich habe gestern, als dieser Vortrag, ich weiß nicht, wie die Dame hieß, von SAP gehalten wurde, dieser Impuls am Anfang, sie hatte doch auch darüber gesprochen, dass Ärzte männlich dargestellt wurden und Frauen weiblich. Und das habe ich mal im Midjourney getestet und ein bisschen rumgespielt. Weil Nurse heißt ja eigentlich auch Krankenpfleger und nicht nur Krankenschwester. Und trotzdem wurden immer Frauen dargestellt. Und bei Ärzten ist ja das Gleiche. Hat mal ein Doktor, dann habe ich mal in DeepL eingegeben, übersetzt mir mal, Arzt und Ärztin, um mal irgendwie dahinter zu steigen. Warum macht der jetzt Männer und Frauen? Da hat er auf einmal gesagt, ich soll Doctor and Physician eingeben. Und das hat dann tatsächlich immer Männer und Frauen gezeigt oder fast ausschließlich Männer und Frauen gezeigt. Aber wenn man eben nur Doktor schreibt, dann kriege ich wirklich fast ausschließlich Männer angezeigt und kaum Frauen. Umgekehrt, wenn ich Physician eingebe, kriege ich doch veränzelt, also tendenziell etwas mehr Frauen, als wenn ich Doktor schreibe. Also es ist ganz, ganz eigenartig mit den Übersetzungen. Alinda? Ja, Susanne, vielleicht eine kritische Frage oder einfach eine Einschätzung von euch, dadurch, dass wir beim Gendern insgesamt jetzt rückläufig sind, als oberster Mufti-Anordnung und viele Industrien, die dem folgen, jetzt aufgrund von irgendwelchen Dingen, die zurückgezogen werden, was ihr für eine Einschätzung habt, vielleicht die Diskussion, ob das Nachlassen im Gendern, was ja vermeintlich jetzt passieren wird, wenn es nicht mehr aufoktiviert wird, dessen Einfluss haben kann auf die Datenauswahl zukünftig. Wenn wir weniger gendern im Netz, wenn wir weniger Genderbegriffe, ob das dann Auswirkungen haben wird auf das, was wir letztlich in der Datenverfügbarkeit haben und hier dann die KI ja auch dementsprechend reagiert in dem, was sie an Ergebnissen findet. Das ist eine sehr spannende Frage. Hat da jemand Ideen dazu? Ich habe eine Hand gerade gesehen. Ganz hinten links im Raum, habe ich richtig geguckt oder nein? Ich glaube, es war ein Kratzen. Ich weiß nicht, da hat da jemand eine Einschätzung. Also ich finde es einfach nur, weil wir immer historische Daten betrachten und die verlagern sich ja gerade wieder, diese Daten. Ob ihr da eine Einschätzung dazu habt, was damit passiert, was ihr euch vorstellt? Weil es ja sehr unterschiedlich ist. Ja, wahrscheinlich. Also es ist eine zu schwierige Frage, die man vielleicht erst mal sacken lassen muss, ein bisschen wirken lassen muss. Also zum einen ist es ja nicht überall, dass man wieder rückwärts geht, dann wird noch ganz viel darüber gesprochen, wie machen wir es denn jetzt richtig, machen wir es so, machen wir es so, machen wir es so, dass es vielleicht auch viel schwieriger am Ende für die KI das so richtig zu klassifizieren und wir versuchen es allgemeiner zu fassen ja mittlerweile von unseren Begriffen her. machen wir ja auch hier zum Beispiel mit Wissensgefährte und Schaffnähe und solchen Sachen. Aber wie will die KI dann richtig klassifizieren, wie sie was darstellt? Das ist halt schon eine Schwierigkeit, die es in Einklang zu bringen gilt und dafür braucht man wahrscheinlich einen gut ausgewogenen Mix an Leuten, die so ein Language-Modell oder auch so eine Bild-KI, die Modelle, die dahinter stecken, dann aufsetzen. Ja, möchte noch jemand, irgendjemand von euch ein Schlusswort sagen? Hat noch jemand was, was er loswerden möchte? Oder wenn es bloß eine Denkanregung für uns alle ist? Nein? Okay. Dann hoffe ich, ihr habt alle ein bisschen was mitgenommen. Mein Ziel war, euch einfach das ein bisschen bewusster zu machen, dass es so ist. Es gibt noch keine Patentlösung, weil noch alles im Fluss ist. Und ich hoffe, dass ihr in der Zukunft euch das alle auch ein bisschen bewusster macht, wenn ihr KI benutzt. Danke, dass ihr alle da wart. 't't